{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2611b293",
   "metadata": {},
   "source": [
    "### 17/03/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1d5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b711e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk version:  3.8.1\n",
      "textblob version:  0.17.1\n",
      "sklearn version:  1.2.1\n",
      "gensim version:  4.3.1\n",
      "spacy version:  3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(\"nltk version: \", nltk.__version__)\n",
    "print(\"textblob version: \", textblob.__version__)\n",
    "print(\"sklearn version: \", sklearn.__version__)\n",
    "print(\"gensim version: \", gensim.__version__)\n",
    "print(\"spacy version: \", spacy.__version__)\n",
    "\n",
    "# nltk version:  3.8.1\n",
    "# textblob version:  0.17.1\n",
    "# sklearn version:  1.2.1\n",
    "# gensim version:  4.3.1\n",
    "# spacy version:  3.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f6f27",
   "metadata": {},
   "source": [
    "## Extracting Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b333fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"John is learning natural language processing\"\n",
    "\n",
    "blob = TextBlob(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7fb033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "natural language processing\n"
     ]
    }
   ],
   "source": [
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca11da7",
   "metadata": {},
   "source": [
    "## Finding Similarity Between Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd03e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = (\n",
    "        \"I like NLP\",\n",
    "        \"I am exploring NLP\",\n",
    "        \"I am a beginner in NLP\",\n",
    "        \"I want to learn NLP\",\n",
    "        \"I like advanced NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4980e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer object\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7714ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = tf_idf_vectorizer.fit_transform(raw_documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94fd3daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'like': 6,\n",
       " 'nlp': 7,\n",
       " 'am': 1,\n",
       " 'exploring': 3,\n",
       " 'beginner': 2,\n",
       " 'in': 4,\n",
       " 'want': 9,\n",
       " 'to': 8,\n",
       " 'learn': 5,\n",
       " 'advanced': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the unique words and their unique keys\n",
    "\n",
    "tf_idf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09e58ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_idf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9a1fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['advanced', 'am', 'beginner', 'exploring', 'in', 'learn', 'like',\n",
       "       'nlp', 'to', 'want'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the unique words\n",
    "\n",
    "tf_idf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e6298c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparse matrix\n",
    "\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1721b42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (5,10) means total sentence is 5 and total unique words are in sentences is 10\n",
    "# 10 represent the features\n",
    "\n",
    "tf_idf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378f0c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.861037  , 0.50854232, 0.        , 0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1015f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.861037  , 0.50854232, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix[0:1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c998ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17682765, 0.14284054, 0.13489366, 0.68374784]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute similarity for first sentence with rest of the sentences\n",
    "\n",
    "# If we clearly observe, the first sentence and last sentence have higher\n",
    "# similarity compared to the rest of the sentences.\n",
    "\n",
    "cosine_similarity(X=tf_idf_matrix[0:1], Y=tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9767cf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17682765, 0.14284054, 0.13489366, 0.68374784]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same result\n",
    "\n",
    "cosine_similarity(X=tf_idf_matrix[0:1].toarray(), Y=tf_idf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3d833",
   "metadata": {},
   "source": [
    "## By Phonetic matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fe19625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will do it later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86676c5a",
   "metadata": {},
   "source": [
    "## Tagging Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754c086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"I love NLP and I will learn NLP in 2 month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f3c80fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "len(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3803a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love NLP and I will learn NLP in 2 month']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text into sentence\n",
    "\n",
    "sent_tokens = sent_tokenize(text=Text, language=\"english\")\n",
    "\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99aeeb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('love', 'VBP'), ('NLP', 'NNP'), ('I', 'PRP'), ('learn', 'VBP'), ('NLP', 'RB'), ('2', 'CD'), ('month', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for word in sent_tokens:\n",
    "    words = word_tokenize(text=word, language=\"english\")\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    # POS tagger\n",
    "    tags = nltk.pos_tag(tokens=words)\n",
    "\n",
    "\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65f41c",
   "metadata": {},
   "source": [
    "## Extract Entities from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db7cd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"John is studying at Stanford University in California\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87a4ed1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'is', 'studying', 'at', 'Stanford', 'University', 'in', 'California']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make word tokens\n",
    "\n",
    "tokens = word_tokenize(text=sent)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d9e09a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('studying', 'VBG'),\n",
       " ('at', 'IN'),\n",
       " ('Stanford', 'NNP'),\n",
       " ('University', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('California', 'NNP')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make pos tag using word tokens\n",
    "\n",
    "tagged_tokens = nltk.pos_tag(tokens=tokens)\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fa167c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'svgling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\formatters.py:342\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    340\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tree\\tree.py:782\u001b[0m, in \u001b[0;36mTree._repr_svg_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_svg_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 782\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msvgling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_tree\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw_tree(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_repr_svg_()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'svgling'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('John', 'NNP')]), ('is', 'VBZ'), ('studying', 'VBG'), ('at', 'IN'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), ('in', 'IN'), Tree('GPE', [('California', 'NNP')])])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(tagged_tokens=tagged_tokens, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed00e1",
   "metadata": {},
   "source": [
    "### Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff12ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f72a9e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read/create a sentence\n",
    "\n",
    "doc = nlp(u'Apple is ready to launch new phone worth $10000 in New york time square ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51ba4c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "10000 42 47 MONEY\n",
      "New york 51 59 GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725eecd",
   "metadata": {},
   "source": [
    "## Extracting Topics from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113ee146",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning\"\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has good exposure into android development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81ea8304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d13a226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning',\n",
       " 'My father is a data scientist and he is nlp expert',\n",
       " 'My sister has good exposure into android development']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = [doc1, doc2, doc3]\n",
    "\n",
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "760a1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and preprocessing\n",
    "\n",
    "def text_cleaner(docs):\n",
    "    preprocess_text = []\n",
    "    for doc in docs:\n",
    "        # Simple text preprocessing like punctuation, number, make lower case etc\n",
    "        text = simple_preprocess(doc=doc)\n",
    "        # Remove the stopwords\n",
    "        text = [word for word in text if word not in all_stopwords]\n",
    "        preprocess_text.append(text)\n",
    "        \n",
    "    return preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1d5107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = text_cleaner(docs=doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "860604e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learning',\n",
       "  'nlp',\n",
       "  'interesting',\n",
       "  'exciting',\n",
       "  'includes',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'deep',\n",
       "  'learning'],\n",
       " ['father', 'data', 'scientist', 'nlp', 'expert'],\n",
       " ['sister', 'good', 'exposure', 'android', 'development']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f7181",
   "metadata": {},
   "source": [
    "#### Preparing document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6b91d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our corpus, where every\n",
    "# unique term is assigned an index.\n",
    "\n",
    "dictionary = corpora.Dictionary(documents=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77caa982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x266cd71aa90>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdc11352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],\n",
       " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting a list of documents (corpus) into Document-Term\n",
    "# Matrix using dictionary prepared above.\n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(document=doc) for doc in result]\n",
    "\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6e1c1",
   "metadata": {},
   "source": [
    "#### LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ccc5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "\n",
    "lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a921bad",
   "metadata": {},
   "source": [
    "lda(\n",
    "    corpus=None,\n",
    "    num_topics=100,\n",
    "    id2word=None,\n",
    "    distributed=False,\n",
    "    chunksize=2000,\n",
    "    passes=1,\n",
    "    update_every=1,\n",
    "    alpha='symmetric',\n",
    "    eta=None,\n",
    "    decay=0.5,\n",
    "    offset=1.0,\n",
    "    eval_every=10,\n",
    "    iterations=50,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=None,\n",
    "    ns_conf=None,\n",
    "    minimum_phi_value=0.01,\n",
    "    per_word_topics=False,\n",
    "    callbacks=None,\n",
    "    dtype=<class 'numpy.float32'>,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9df9e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and Training LDA model on the document term matrix for 3 topics.\n",
    "\n",
    "lda_model = lda(corpus=doc_term_matrix, num_topics=3, id2word=dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "906b0d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.233*\"learning\" + 0.093*\"exciting\" + 0.093*\"deep\" + 0.093*\"machine\" + 0.093*\"includes\" + 0.093*\"interesting\" + 0.093*\"nlp\" + 0.023*\"father\" + 0.023*\"data\" + 0.023*\"expert\"'), (1, '0.129*\"android\" + 0.129*\"sister\" + 0.129*\"good\" + 0.129*\"exposure\" + 0.129*\"development\" + 0.032*\"nlp\" + 0.032*\"father\" + 0.032*\"scientist\" + 0.032*\"data\" + 0.032*\"expert\"'), (2, '0.129*\"nlp\" + 0.129*\"father\" + 0.129*\"data\" + 0.129*\"scientist\" + 0.129*\"expert\" + 0.032*\"good\" + 0.032*\"exposure\" + 0.032*\"development\" + 0.032*\"android\" + 0.032*\"sister\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99813f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.233*\"learning\" + 0.093*\"exciting\" + 0.093*\"deep\" + 0.093*\"machine\" + 0.093*\"includes\" + 0.093*\"interesting\" + 0.093*\"nlp\" + 0.023*\"father\" + 0.023*\"data\" + 0.023*\"expert\"')\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topics()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a15ed4a",
   "metadata": {},
   "source": [
    "`All the weights associated with the topics from the sentence seem\n",
    "almost similar. You can perform this on huge data to extract significant\n",
    "topics. The whole idea to implement this on sample data is to make you\n",
    "familiar with it, and you can use the same code snippet to perform on the\n",
    "huge data for significant results and insights.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc99175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
