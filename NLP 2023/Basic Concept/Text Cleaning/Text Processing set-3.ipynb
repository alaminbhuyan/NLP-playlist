{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2168a0c6-3902-4335-b6dd-84009a8f2e4f",
   "metadata": {},
   "source": [
    "### Text Cleaning in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd5b71d-c849-4c18-ad59-c856e4ca8386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am writing some very basic english sentences', \"I'm just writing it for the demo PURPOSE to make audience understand the basics .\", 'The point is to _learn HOW it works_ on #simple # data.']\n"
     ]
    }
   ],
   "source": [
    "#Creating bunch of sentences\n",
    "\n",
    "raw_docs = [\"I am writing some very basic english sentences\",\n",
    "\"I'm just writing it for the demo PURPOSE to make audience understand the basics .\",\n",
    "\"The point is to _learn HOW it works_ on #simple # data.\"]\n",
    "\n",
    "print(raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d6e1e-0d84-4571-8971-a15128af6deb",
   "metadata": {},
   "source": [
    "### Step:1- Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd1aa48-614e-455a-959a-52879b95e958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i am writing some very basic english sentences', \"i'm just writing it for the demo purpose to make audience understand the basics .\", 'the point is to _learn how it works_ on #simple # data.']\n"
     ]
    }
   ],
   "source": [
    "raw_docs2 = [doc.lower() for doc in raw_docs]\n",
    "\n",
    "print(raw_docs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95687b0-91e3-491b-8e31-0ef30dba4ca2",
   "metadata": {},
   "source": [
    "### Step:2- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04460f93-e03c-4b5f-ae5b-07cc088a1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79287d4-fde5-41d7-a881-61cafd604262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'am', 'writing', 'some', 'very', 'basic', 'english', 'sentences'], ['i', \"'m\", 'just', 'writing', 'it', 'for', 'the', 'demo', 'purpose', 'to', 'make', 'audience', 'understand', 'the', 'basics', '.'], ['the', 'point', 'is', 'to', '_learn', 'how', 'it', 'works_', 'on', '#', 'simple', '#', 'data', '.']]\n",
      "####################################################################################################\n",
      "[['i am writing some very basic english sentences'], [\"i'm just writing it for the demo purpose to make audience understand the basics .\"], ['the point is to _learn how it works_ on #simple # data.']]\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "\n",
    "word_tokenized_docs = [word_tokenize(text=doc) for doc in raw_docs2]\n",
    "print(word_tokenized_docs)\n",
    "\n",
    "print(\"#\"*100)\n",
    "\n",
    "sent_tokenized_docs = [sent_tokenize(text=doc) for doc in raw_docs2]\n",
    "print(sent_tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82c4c6-dd1a-4b59-aed3-e229b4b23883",
   "metadata": {},
   "source": [
    "### Step:3- Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ed31f9-a4f3-42dc-88a3-d733c8540fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e52a0fd-de7a-4885-8268-c5bd72a82038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'am', 'writing', 'some', 'very', 'basic', 'english', 'sentences'], ['i', 'm', 'just', 'writing', 'it', 'for', 'the', 'demo', 'purpose', 'to', 'make', 'audience', 'understand', 'the', 'basics'], ['the', 'point', 'is', 'to', 'learn', 'how', 'it', 'works', 'on', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(pattern='[%s]' % re.escape(pattern=string.punctuation))\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for word in word_tokenized_docs:\n",
    "    new_lis = []\n",
    "    for word_token in word:\n",
    "        new_token = regex.sub(u'', word_token)\n",
    "        if not new_token == u'':\n",
    "            new_lis.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_lis)\n",
    "\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37ffdb-c18f-4c72-998f-3877bed8fb71",
   "metadata": {},
   "source": [
    "### Step:4- Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f126b2-d835-4441-b70d-4daf17964e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf3dcfa0-1449-419e-8227-26999d7c6e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['writing', 'basic', 'english', 'sentences'], ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basics'], ['point', 'learn', 'works', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33da66a-d499-4575-8c5e-a067e50aaf91",
   "metadata": {},
   "source": [
    "### Step:5- Stemming and Lemmantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a730ea6-e606-44dd-aca8-2cc11b4c5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7650a890-3f90-4052-b177-675365c1c4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['writing', 'basic', 'english', 'sentence'], ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basic'], ['point', 'learn', 'work', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        #final_doc.append(porter.stem(word))\n",
    "        final_doc.append(wordnet.lemmatize(word))\n",
    "    \n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print(preprocessed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
